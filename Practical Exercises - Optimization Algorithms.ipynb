{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d91c1075",
   "metadata": {},
   "source": [
    "# Practical Exercises - Optimization Algorithms\n",
    "## Hands-On Problems for Interview Preparation\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Purpose\n",
    "\n",
    "These exercises will help you:\n",
    "1. Solidify understanding through practice\n",
    "2. Prepare for coding interviews\n",
    "3. Build confidence in implementation\n",
    "4. Develop debugging skills\n",
    "\n",
    "**Difficulty levels:** ‚≠ê Easy | ‚≠ê‚≠ê Medium | ‚≠ê‚≠ê‚≠ê Hard\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: Gradient Computation\n",
    "\n",
    "### Exercise 1.1: Simple Derivatives ‚≠ê\n",
    "**Task:** Compute gradients analytically for these functions:\n",
    "\n",
    "a) `f(x) = x¬≤`\n",
    "   - Compute: ‚àÇf/‚àÇx = ?\n",
    "   - Verify: At x=3, gradient = ?\n",
    "\n",
    "b) `f(x, y) = x¬≤ + 3xy + y¬≤`\n",
    "   - Compute: ‚àÇf/‚àÇx = ?\n",
    "   - Compute: ‚àÇf/‚àÇy = ?\n",
    "\n",
    "c) `f(Œ∏) = ||Œ∏ - [1, 2, 3]||¬≤` where Œ∏ ‚àà ‚Ñù¬≥\n",
    "   - Compute: ‚àáf(Œ∏) = ?\n",
    "\n",
    "**Solution hints in `MATHEMATICAL_DERIVATIONS.md`**\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 1.2: Numerical Gradient ‚≠ê‚≠ê\n",
    "**Task:** Implement numerical gradient computation\n",
    "\n",
    "```python\n",
    "def numerical_gradient(f, x, epsilon=1e-5):\n",
    "    \"\"\"\n",
    "    Compute gradient of f at x numerically.\n",
    "    \n",
    "    Args:\n",
    "        f: Function to differentiate\n",
    "        x: Point at which to compute gradient (numpy array)\n",
    "        epsilon: Small step size\n",
    "    \n",
    "    Returns:\n",
    "        grad: Numerical gradient (same shape as x)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# Test cases\n",
    "def f1(x):\n",
    "    return np.sum(x**2)\n",
    "\n",
    "def f2(x):\n",
    "    return np.sum((x - np.array([1, 2, 3]))**2)\n",
    "\n",
    "x_test = np.array([2.0, 3.0, 4.0])\n",
    "grad1 = numerical_gradient(f1, x_test)\n",
    "grad2 = numerical_gradient(f2, x_test)\n",
    "\n",
    "print(f\"Gradient of f1 at {x_test}: {grad1}\")\n",
    "print(f\"Expected: {2*x_test}\")  # Should be [4, 6, 8]\n",
    "\n",
    "print(f\"\\nGradient of f2 at {x_test}: {grad2}\")\n",
    "print(f\"Expected: {2*(x_test - np.array([1,2,3]))}\")  # Should be [2, 2, 2]\n",
    "```\n",
    "\n",
    "**Expected output:**\n",
    "```\n",
    "Gradient of f1 at [2. 3. 4.]: [4. 6. 8.]\n",
    "Expected: [4. 6. 8.]\n",
    "\n",
    "Gradient of f2 at [2. 3. 4.]: [2. 2. 2.]\n",
    "Expected: [2. 2. 2.]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 1.3: Gradient Descent from Scratch ‚≠ê‚≠ê\n",
    "**Task:** Implement basic gradient descent\n",
    "\n",
    "```python\n",
    "def gradient_descent(f, grad_f, x0, learning_rate=0.1, n_iterations=100):\n",
    "    \"\"\"\n",
    "    Minimize function f using gradient descent.\n",
    "    \n",
    "    Args:\n",
    "        f: Function to minimize\n",
    "        grad_f: Function that computes gradient of f\n",
    "        x0: Initial point\n",
    "        learning_rate: Step size\n",
    "        n_iterations: Number of iterations\n",
    "    \n",
    "    Returns:\n",
    "        x: Final point\n",
    "        history: List of (x, f(x)) at each iteration\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# Test: Minimize f(x, y) = (x-1)¬≤ + (y-2)¬≤\n",
    "def f(x):\n",
    "    return (x[0] - 1)**2 + (x[1] - 2)**2\n",
    "\n",
    "def grad_f(x):\n",
    "    return np.array([2*(x[0] - 1), 2*(x[1] - 2)])\n",
    "\n",
    "x0 = np.array([5.0, 5.0])\n",
    "x_final, history = gradient_descent(f, grad_f, x0, learning_rate=0.1, n_iterations=50)\n",
    "\n",
    "print(f\"Starting point: {x0}\")\n",
    "print(f\"Optimal point: [1, 2]\")\n",
    "print(f\"Found point: {x_final}\")\n",
    "print(f\"Distance from optimal: {np.linalg.norm(x_final - np.array([1, 2])):.6f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2: Neural Network Components\n",
    "\n",
    "### Exercise 2.1: Activation Functions ‚≠ê\n",
    "**Task:** Implement common activation functions and their derivatives\n",
    "\n",
    "```python\n",
    "def tanh(z):\n",
    "    \"\"\"Hyperbolic tangent activation.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "def tanh_derivative(z):\n",
    "    \"\"\"Derivative of tanh.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # Hint: tanh'(z) = 1 - tanh¬≤(z)\n",
    "    pass\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\"ReLU activation.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "def relu_derivative(z):\n",
    "    \"\"\"Derivative of ReLU.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    \"\"\"Derivative of sigmoid.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # Hint: œÉ'(z) = œÉ(z)(1 - œÉ(z))\n",
    "    pass\n",
    "\n",
    "# Test cases\n",
    "z_test = np.array([-2, -1, 0, 1, 2])\n",
    "print(f\"tanh({z_test}) = {tanh(z_test)}\")\n",
    "print(f\"ReLU({z_test}) = {relu(z_test)}\")\n",
    "print(f\"sigmoid({z_test}) = {sigmoid(z_test)}\")\n",
    "```\n",
    "\n",
    "**Expected behavior:**\n",
    "- tanh maps to [-1, 1]\n",
    "- ReLU zeros out negatives\n",
    "- sigmoid maps to [0, 1]\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 2.2: Forward Propagation ‚≠ê‚≠ê\n",
    "**Task:** Implement forward pass for a 2-layer network\n",
    "\n",
    "```python\n",
    "def forward_pass(X, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Compute forward pass for 2-layer network.\n",
    "    \n",
    "    Architecture:\n",
    "        h1 = tanh(W1¬∑X + b1)\n",
    "        y = W2¬∑h1 + b2\n",
    "    \n",
    "    Args:\n",
    "        X: Input (batch_size, input_dim)\n",
    "        W1: First layer weights (hidden_dim, input_dim)\n",
    "        b1: First layer bias (hidden_dim,)\n",
    "        W2: Second layer weights (output_dim, hidden_dim)\n",
    "        b2: Second layer bias (output_dim,)\n",
    "    \n",
    "    Returns:\n",
    "        y: Output (batch_size, output_dim)\n",
    "        cache: Dictionary with intermediate values for backprop\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # Remember to save z1, h1 for backward pass!\n",
    "    pass\n",
    "\n",
    "# Test with random data\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(5, 10)  # 5 samples, 10 features\n",
    "W1 = np.random.randn(20, 10)  # 10 ‚Üí 20\n",
    "b1 = np.random.randn(20)\n",
    "W2 = np.random.randn(2, 20)  # 20 ‚Üí 2\n",
    "b2 = np.random.randn(2)\n",
    "\n",
    "y, cache = forward_pass(X, W1, b1, W2, b2)\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(f\"Cache keys: {cache.keys()}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 2.3: Backward Propagation ‚≠ê‚≠ê‚≠ê\n",
    "**Task:** Implement backward pass (backpropagation)\n",
    "\n",
    "```python\n",
    "def backward_pass(dy, cache, W2):\n",
    "    \"\"\"\n",
    "    Compute gradients using backpropagation.\n",
    "    \n",
    "    Args:\n",
    "        dy: Gradient of loss w.r.t output (batch_size, output_dim)\n",
    "        cache: Dictionary from forward pass\n",
    "        W2: Second layer weights\n",
    "    \n",
    "    Returns:\n",
    "        grads: Dictionary with gradients\n",
    "            - dW2: Gradient w.r.t W2\n",
    "            - db2: Gradient w.r.t b2\n",
    "            - dW1: Gradient w.r.t W1\n",
    "            - db1: Gradient w.r.t b1\n",
    "    \"\"\"\n",
    "    # Extract cached values\n",
    "    X = cache['X']\n",
    "    z1 = cache['z1']\n",
    "    h1 = cache['h1']\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # Hints:\n",
    "    # 1. Œ¥2 = dy\n",
    "    # 2. dW2 = Œ¥2¬∑h1·µÄ\n",
    "    # 3. db2 = sum(Œ¥2)\n",
    "    # 4. Œ¥1 = (W2·µÄ¬∑Œ¥2) ‚äô tanh'(z1)\n",
    "    # 5. dW1 = Œ¥1¬∑X·µÄ\n",
    "    # 6. db1 = sum(Œ¥1)\n",
    "    \n",
    "    grads = {}\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return grads\n",
    "\n",
    "# Test (continuing from Exercise 2.2)\n",
    "dy = np.random.randn(5, 2)  # Random gradient from loss\n",
    "grads = backward_pass(dy, cache, W2)\n",
    "\n",
    "print(\"Gradient shapes:\")\n",
    "for key, value in grads.items():\n",
    "    print(f\"  {key}: {value.shape}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Part 3: Complete Training Loop\n",
    "\n",
    "### Exercise 3.1: MSE Loss ‚≠ê\n",
    "**Task:** Implement mean squared error loss and its gradient\n",
    "\n",
    "```python\n",
    "def mse_loss(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Compute mean squared error.\n",
    "    \n",
    "    Args:\n",
    "        y_pred: Predictions (batch_size, output_dim)\n",
    "        y_true: True values (batch_size, output_dim)\n",
    "    \n",
    "    Returns:\n",
    "        loss: Scalar MSE value\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "def mse_gradient(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Compute gradient of MSE w.r.t predictions.\n",
    "    \n",
    "    Args:\n",
    "        y_pred: Predictions (batch_size, output_dim)\n",
    "        y_true: True values (batch_size, output_dim)\n",
    "    \n",
    "    Returns:\n",
    "        grad: Gradient (batch_size, output_dim)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # Hint: ‚àÇMSE/‚àÇy_pred = (2/n)(y_pred - y_true)\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "y_pred = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
    "y_true = np.array([[1.5, 2.5], [2.5, 3.5]])\n",
    "\n",
    "loss = mse_loss(y_pred, y_true)\n",
    "grad = mse_gradient(y_pred, y_true)\n",
    "\n",
    "print(f\"MSE Loss: {loss:.4f}\")\n",
    "print(f\"Gradient shape: {grad.shape}\")\n",
    "print(f\"Gradient:\\n{grad}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 3.2: Mini-Batch Training ‚≠ê‚≠ê‚≠ê\n",
    "**Task:** Implement complete training loop with mini-batches\n",
    "\n",
    "```python\n",
    "def train_network(X_train, Y_train, X_val, Y_val, \n",
    "                  hidden_dim=20, learning_rate=0.01, \n",
    "                  batch_size=32, n_epochs=100):\n",
    "    \"\"\"\n",
    "    Train a 2-layer neural network.\n",
    "    \n",
    "    Args:\n",
    "        X_train, Y_train: Training data\n",
    "        X_val, Y_val: Validation data\n",
    "        hidden_dim: Size of hidden layer\n",
    "        learning_rate: Step size\n",
    "        batch_size: Mini-batch size\n",
    "        n_epochs: Number of epochs\n",
    "    \n",
    "    Returns:\n",
    "        params: Trained parameters\n",
    "        history: Training history\n",
    "    \"\"\"\n",
    "    # Initialize parameters\n",
    "    input_dim = X_train.shape[1]\n",
    "    output_dim = Y_train.shape[1]\n",
    "    \n",
    "    W1 = np.random.randn(hidden_dim, input_dim) * 0.01\n",
    "    b1 = np.zeros(hidden_dim)\n",
    "    W2 = np.random.randn(output_dim, hidden_dim) * 0.01\n",
    "    b2 = np.zeros(output_dim)\n",
    "    \n",
    "    n_train = X_train.shape[0]\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # Implement training loop:\n",
    "    # 1. For each epoch:\n",
    "    #    2. Shuffle training data\n",
    "    #    3. For each mini-batch:\n",
    "    #       4. Forward pass\n",
    "    #       5. Compute loss\n",
    "    #       6. Backward pass\n",
    "    #       7. Update parameters\n",
    "    #    8. Evaluate on validation set\n",
    "    #    9. Store losses\n",
    "    \n",
    "    params = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "    return params, history\n",
    "\n",
    "# Test with synthetic regression data\n",
    "np.random.seed(42)\n",
    "X_train = np.random.randn(500, 10)\n",
    "Y_train = X_train @ np.random.randn(10, 2) + np.random.randn(500, 2) * 0.1\n",
    "\n",
    "X_val = np.random.randn(100, 10)\n",
    "Y_val = X_val @ np.random.randn(10, 2) + np.random.randn(100, 2) * 0.1\n",
    "\n",
    "params, history = train_network(X_train, Y_train, X_val, Y_val, n_epochs=50)\n",
    "\n",
    "# Plot training curves\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history['train_loss'], label='Train')\n",
    "plt.plot(history['val_loss'], label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.title('Training Curves')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Part 4: Debugging and Analysis\n",
    "\n",
    "### Exercise 4.1: Gradient Checking ‚≠ê‚≠ê\n",
    "**Task:** Verify your backpropagation implementation\n",
    "\n",
    "```python\n",
    "def gradient_check(X, Y, params, epsilon=1e-5):\n",
    "    \"\"\"\n",
    "    Verify backpropagation gradients using numerical gradients.\n",
    "    \n",
    "    Args:\n",
    "        X: Input data\n",
    "        Y: Target data\n",
    "        params: Network parameters\n",
    "        epsilon: Small perturbation\n",
    "    \n",
    "    Returns:\n",
    "        max_diff: Maximum relative error\n",
    "        passed: Boolean indicating if check passed\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # 1. Compute analytical gradients using backprop\n",
    "    # 2. Compute numerical gradients for each parameter\n",
    "    # 3. Compare and compute relative error\n",
    "    # 4. Return max error and whether test passed\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(10, 5)\n",
    "Y = np.random.randn(10, 2)\n",
    "params = {\n",
    "    'W1': np.random.randn(8, 5) * 0.01,\n",
    "    'b1': np.zeros(8),\n",
    "    'W2': np.random.randn(2, 8) * 0.01,\n",
    "    'b2': np.zeros(2)\n",
    "}\n",
    "\n",
    "max_diff, passed = gradient_check(X, Y, params)\n",
    "print(f\"Maximum relative error: {max_diff:.2e}\")\n",
    "print(f\"Gradient check: {'‚úì PASSED' if passed else '‚úó FAILED'}\")\n",
    "```\n",
    "\n",
    "**Success criteria:** Relative error < 1e-7\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 4.2: Learning Rate Experimentation ‚≠ê‚≠ê\n",
    "**Task:** Analyze effect of different learning rates\n",
    "\n",
    "```python\n",
    "def compare_learning_rates(X_train, Y_train, learning_rates):\n",
    "    \"\"\"\n",
    "    Train networks with different learning rates and compare.\n",
    "    \n",
    "    Args:\n",
    "        X_train, Y_train: Training data\n",
    "        learning_rates: List of learning rates to try\n",
    "    \n",
    "    Returns:\n",
    "        results: Dictionary with results for each learning rate\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for lr in learning_rates:\n",
    "        # YOUR CODE HERE\n",
    "        # Train network with this learning rate\n",
    "        # Store final loss and convergence behavior\n",
    "        pass\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test\n",
    "learning_rates = [0.001, 0.01, 0.1, 1.0]\n",
    "results = compare_learning_rates(X_train, Y_train, learning_rates)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 4))\n",
    "for lr, history in results.items():\n",
    "    plt.plot(history, label=f'Œ±={lr}')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.title('Learning Rate Comparison')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Questions to answer:**\n",
    "1. Which learning rate converges fastest?\n",
    "2. Which learning rates diverge?\n",
    "3. What's the optimal range?\n",
    "\n",
    "---\n",
    "\n",
    "## Part 5: Advanced Challenges\n",
    "\n",
    "### Exercise 5.1: Momentum ‚≠ê‚≠ê‚≠ê\n",
    "**Task:** Implement SGD with momentum\n",
    "\n",
    "```python\n",
    "def sgd_with_momentum(params, grads, velocities, learning_rate, momentum=0.9):\n",
    "    \"\"\"\n",
    "    Update parameters using SGD with momentum.\n",
    "    \n",
    "    v_t = Œ≤¬∑v_{t-1} + ‚àáL\n",
    "    Œ∏_t = Œ∏_{t-1} - Œ±¬∑v_t\n",
    "    \n",
    "    Args:\n",
    "        params: Current parameters\n",
    "        grads: Gradients\n",
    "        velocities: Velocity terms (updated in-place)\n",
    "        learning_rate: Step size\n",
    "        momentum: Momentum coefficient\n",
    "    \n",
    "    Returns:\n",
    "        Updated params and velocities\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# Compare with vanilla SGD\n",
    "# Plot convergence curves\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 5.2: Adam Optimizer ‚≠ê‚≠ê‚≠ê\n",
    "**Task:** Implement Adam optimizer from scratch\n",
    "\n",
    "```python\n",
    "def adam_optimizer(params, grads, m, v, t, learning_rate=0.001, \n",
    "                   beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Update parameters using Adam optimizer.\n",
    "    \n",
    "    m_t = Œ≤‚ÇÅ¬∑m_{t-1} + (1-Œ≤‚ÇÅ)¬∑‚àáL\n",
    "    v_t = Œ≤‚ÇÇ¬∑v_{t-1} + (1-Œ≤‚ÇÇ)¬∑‚àáL¬≤\n",
    "    mÃÇ_t = m_t / (1 - Œ≤‚ÇÅ^t)\n",
    "    vÃÇ_t = v_t / (1 - Œ≤‚ÇÇ^t)\n",
    "    Œ∏_t = Œ∏_{t-1} - Œ±¬∑mÃÇ_t / (‚àövÃÇ_t + Œµ)\n",
    "    \n",
    "    Args:\n",
    "        params: Current parameters\n",
    "        grads: Gradients\n",
    "        m: First moment estimates (updated in-place)\n",
    "        v: Second moment estimates (updated in-place)\n",
    "        t: Time step\n",
    "        learning_rate: Step size\n",
    "        beta1: First moment decay rate\n",
    "        beta2: Second moment decay rate\n",
    "        epsilon: Small constant for numerical stability\n",
    "    \n",
    "    Returns:\n",
    "        Updated params, m, v\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 5.3: Real Dataset Application ‚≠ê‚≠ê‚≠ê\n",
    "**Task:** Apply to MNIST digit recognition\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# One-hot encode labels\n",
    "y_onehot = np.zeros((y.size, 10))\n",
    "y_onehot[np.arange(y.size), y] = 1\n",
    "\n",
    "# Split and standardize\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, y_onehot, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# YOUR TASK:\n",
    "# 1. Design appropriate architecture (64 ‚Üí ? ‚Üí ? ‚Üí 10)\n",
    "# 2. Train using your implementation\n",
    "# 3. Achieve >90% accuracy\n",
    "# 4. Plot confusion matrix\n",
    "# 5. Analyze misclassified examples\n",
    "\n",
    "# Evaluation\n",
    "def accuracy(Y_pred, Y_true):\n",
    "    pred_classes = np.argmax(Y_pred, axis=1)\n",
    "    true_classes = np.argmax(Y_true, axis=1)\n",
    "    return np.mean(pred_classes == true_classes)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "```\n",
    "\n",
    "**Target:** >90% test accuracy\n",
    "\n",
    "---\n",
    "\n",
    "## Part 6: Interview-Style Questions\n",
    "\n",
    "### Question 6.1: Conceptual ‚≠ê\n",
    "**Q:** Why do we use non-linear activation functions?\n",
    "\n",
    "**Your answer:**\n",
    "```\n",
    "[Write your explanation here]\n",
    "```\n",
    "\n",
    "**Key points to cover:**\n",
    "- Without non-linearity, network is just linear transformation\n",
    "- Multiple linear layers = single linear layer\n",
    "- Non-linearity enables learning complex functions\n",
    "- Examples: XOR problem\n",
    "\n",
    "---\n",
    "\n",
    "### Question 6.2: Mathematical ‚≠ê‚≠ê\n",
    "**Q:** Derive the gradient for this loss function:\n",
    "```\n",
    "L = -‚àë·µ¢ y·µ¢ log(≈∑·µ¢)  (cross-entropy loss)\n",
    "```\n",
    "\n",
    "**Your derivation:**\n",
    "```\n",
    "‚àÇL/‚àÇ≈∑‚±º = ?\n",
    "\n",
    "[Show your work here]\n",
    "```\n",
    "\n",
    "**Answer:** ‚àÇL/‚àÇ≈∑‚±º = -y‚±º/≈∑‚±º\n",
    "\n",
    "---\n",
    "\n",
    "### Question 6.3: Debugging ‚≠ê‚≠ê‚≠ê\n",
    "**Q:** Your network's training loss is stuck at a high value. List 5 potential causes and how to diagnose each.\n",
    "\n",
    "**Your answer:**\n",
    "```\n",
    "1. [Cause + diagnostic approach]\n",
    "2. [Cause + diagnostic approach]\n",
    "3. [Cause + diagnostic approach]\n",
    "4. [Cause + diagnostic approach]\n",
    "5. [Cause + diagnostic approach]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Question 6.4: Coding Challenge ‚≠ê‚≠ê‚≠ê\n",
    "**Q:** Implement this function in 15 minutes:\n",
    "\n",
    "```python\n",
    "def find_optimal_learning_rate(f, grad_f, x0, lr_min=1e-5, lr_max=1.0, n_steps=50):\n",
    "    \"\"\"\n",
    "    Find optimal learning rate using a learning rate range test.\n",
    "    \n",
    "    Try learning rates from lr_min to lr_max (log scale).\n",
    "    For each, take n_steps and record final loss.\n",
    "    Return the learning rate that gave lowest loss.\n",
    "    \n",
    "    Args:\n",
    "        f: Function to minimize\n",
    "        grad_f: Gradient function\n",
    "        x0: Starting point\n",
    "        lr_min: Minimum learning rate to try\n",
    "        lr_max: Maximum learning rate to try\n",
    "        n_steps: Steps per learning rate\n",
    "    \n",
    "    Returns:\n",
    "        best_lr: Optimal learning rate\n",
    "        results: Dictionary mapping learning rate to final loss\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE (15 minutes)\n",
    "    pass\n",
    "\n",
    "# Test it\n",
    "def test_quadratic(x):\n",
    "    return np.sum((x - np.array([1, 2]))**2)\n",
    "\n",
    "def test_quadratic_grad(x):\n",
    "    return 2*(x - np.array([1, 2]))\n",
    "\n",
    "best_lr, results = find_optimal_learning_rate(\n",
    "    test_quadratic, test_quadratic_grad, \n",
    "    np.array([10.0, 10.0])\n",
    ")\n",
    "\n",
    "print(f\"Best learning rate: {best_lr}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Solutions and Hints\n",
    "\n",
    "### Getting Started\n",
    "1. Start with easy exercises (‚≠ê)\n",
    "2. Use `optimization_tutorial.py` as reference\n",
    "3. Test incrementally (don't write everything at once)\n",
    "4. Use print statements for debugging\n",
    "5. Compare with expected outputs\n",
    "\n",
    "### Common Mistakes\n",
    "- Forgetting to divide by batch size in gradients\n",
    "- Wrong matrix dimensions in backprop\n",
    "- Not initializing velocities for momentum\n",
    "- Numerical instability in loss functions\n",
    "\n",
    "### Testing Your Code\n",
    "```python\n",
    "# Always test with simple cases where you know the answer\n",
    "def simple_test():\n",
    "    # Linear function: f(x) = 2x + 1\n",
    "    # Gradient should be exactly 2\n",
    "    def f(x):\n",
    "        return 2*x + 1\n",
    "    \n",
    "    def grad_f(x):\n",
    "        return 2\n",
    "    \n",
    "    # Test your gradient descent\n",
    "    x0 = 10.0\n",
    "    x_opt = gradient_descent(f, grad_f, x0, lr=0.1, n_iter=100)\n",
    "    \n",
    "    # Should converge to some minimum\n",
    "    # (though this function has no minimum, it should at least decrease)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "After completing these exercises:\n",
    "\n",
    "1. ‚úÖ **Review Solutions** - Check against `optimization_tutorial.py`\n",
    "2. ‚úÖ **Benchmark Performance** - Time your implementations\n",
    "3. ‚úÖ **Add Features** - Implement batch normalization, dropout\n",
    "4. ‚úÖ **Scale Up** - Try larger networks and datasets\n",
    "5. ‚úÖ **Learn Frameworks** - Translate knowledge to PyTorch/TensorFlow\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluation Rubric\n",
    "\n",
    "### For Self-Assessment\n",
    "\n",
    "**Basic Understanding** (‚≠ê)\n",
    "- [ ] Can implement gradient descent\n",
    "- [ ] Understands forward propagation\n",
    "- [ ] Can compute simple derivatives\n",
    "\n",
    "**Intermediate Skills** (‚≠ê‚≠ê)\n",
    "- [ ] Can implement backpropagation\n",
    "- [ ] Understands optimization algorithms\n",
    "- [ ] Can debug training issues\n",
    "\n",
    "**Advanced Proficiency** (‚≠ê‚≠ê‚≠ê)\n",
    "- [ ] Can implement modern optimizers\n",
    "- [ ] Applies to real datasets\n",
    "- [ ] Optimizes performance\n",
    "- [ ] Explains trade-offs clearly\n",
    "\n",
    "---\n",
    "\n",
    "**Good luck with your practice!** üöÄ\n",
    "\n",
    "*Remember: The goal isn't to memorize code, but to deeply understand the concepts so you can implement and debug confidently in any interview or project.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d373b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
