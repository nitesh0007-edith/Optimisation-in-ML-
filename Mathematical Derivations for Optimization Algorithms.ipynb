{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59372cd0",
   "metadata": {},
   "source": [
    "# Mathematical Derivations for Optimization Algorithms\n",
    "## Detailed Proofs and Intuitions\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Gradient Descent Derivation](#gradient-descent-derivation)\n",
    "2. [Backpropagation Algorithm](#backpropagation-algorithm)\n",
    "3. [Matrix Calculus](#matrix-calculus)\n",
    "4. [Chain Rule for Neural Networks](#chain-rule-for-neural-networks)\n",
    "5. [Convergence Analysis](#convergence-analysis)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Gradient Descent Derivation\n",
    "\n",
    "### 1.1 First-Order Taylor Expansion\n",
    "\n",
    "For a differentiable function L(θ), we can approximate it around point θ₀:\n",
    "\n",
    "```\n",
    "L(θ₀ + Δθ) ≈ L(θ₀) + ∇L(θ₀)ᵀ·Δθ\n",
    "```\n",
    "\n",
    "**Goal**: Choose Δθ to minimize L(θ₀ + Δθ)\n",
    "\n",
    "### 1.2 Finding the Descent Direction\n",
    "\n",
    "To find the direction that decreases L the most, consider:\n",
    "\n",
    "```\n",
    "L(θ₀ + αd) ≈ L(θ₀) + α·∇L(θ₀)ᵀ·d\n",
    "```\n",
    "\n",
    "Where:\n",
    "- α > 0 is step size\n",
    "- d is unit direction vector (||d|| = 1)\n",
    "\n",
    "**Minimize**: ∇L(θ₀)ᵀ·d\n",
    "\n",
    "Using Cauchy-Schwarz inequality:\n",
    "```\n",
    "∇L(θ₀)ᵀ·d ≥ -||∇L(θ₀)|| · ||d|| = -||∇L(θ₀)||\n",
    "```\n",
    "\n",
    "Equality holds when:\n",
    "```\n",
    "d = -∇L(θ₀) / ||∇L(θ₀)||\n",
    "```\n",
    "\n",
    "**Therefore**, the steepest descent direction is **-∇L(θ₀)**.\n",
    "\n",
    "### 1.3 Update Rule\n",
    "\n",
    "```\n",
    "θ_{t+1} = θ_t - α·∇L(θ_t)\n",
    "```\n",
    "\n",
    "Where α is the learning rate (step size).\n",
    "\n",
    "### 1.4 Intuition\n",
    "\n",
    "Think of standing on a mountain:\n",
    "- ∇L(θ) points uphill (direction of steepest ascent)\n",
    "- -∇L(θ) points downhill (direction of steepest descent)\n",
    "- We take steps downhill to reach the valley (minimum)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Backpropagation Algorithm\n",
    "\n",
    "### 2.1 Problem Setup\n",
    "\n",
    "Given:\n",
    "- Neural network: f(x; θ)\n",
    "- Loss function: L = ||f(x; θ) - y||²\n",
    "- Goal: Compute ∇_θ L efficiently\n",
    "\n",
    "### 2.2 Forward Pass Equations\n",
    "\n",
    "For a 4-layer network:\n",
    "\n",
    "```\n",
    "Layer 0 (Input):\n",
    "h₀ = x\n",
    "\n",
    "Layer 1:\n",
    "z₁ = W₁·h₀ + b₁\n",
    "h₁ = tanh(z₁)\n",
    "\n",
    "Layer 2:\n",
    "z₂ = W₂·h₁ + b₂\n",
    "h₂ = tanh(z₂)\n",
    "\n",
    "Layer 3:\n",
    "z₃ = W₃·h₂ + b₃\n",
    "h₃ = tanh(z₃)\n",
    "\n",
    "Layer 4 (Output):\n",
    "z₄ = W₄·h₃ + b₄\n",
    "ŷ = z₄\n",
    "```\n",
    "\n",
    "### 2.3 Loss Function\n",
    "\n",
    "```\n",
    "L = (1/2)||ŷ - y||² = (1/2)Σᵢ(ŷᵢ - yᵢ)²\n",
    "```\n",
    "\n",
    "### 2.4 Backward Pass - Output Layer\n",
    "\n",
    "Compute ∂L/∂z₄:\n",
    "\n",
    "```\n",
    "∂L/∂z₄ = ∂L/∂ŷ · ∂ŷ/∂z₄\n",
    "       = (ŷ - y) · I\n",
    "       = ŷ - y\n",
    "\n",
    "Define: δ₄ = ŷ - y\n",
    "```\n",
    "\n",
    "Gradients for output layer parameters:\n",
    "\n",
    "```\n",
    "∂L/∂W₄ = δ₄ ⊗ h₃ᵀ = δ₄·h₃ᵀ\n",
    "∂L/∂b₄ = δ₄\n",
    "```\n",
    "\n",
    "**Matrix dimensions**:\n",
    "- δ₄: (n_out, 1)\n",
    "- h₃: (n₃, 1)\n",
    "- ∂L/∂W₄: (n_out, n₃)\n",
    "\n",
    "### 2.5 Backward Pass - Hidden Layer 3\n",
    "\n",
    "Compute ∂L/∂z₃:\n",
    "\n",
    "```\n",
    "∂L/∂z₃ = ∂L/∂z₄ · ∂z₄/∂h₃ · ∂h₃/∂z₃\n",
    "       = δ₄ · W₄ · tanh'(z₃)\n",
    "\n",
    "Since tanh'(z) = 1 - tanh²(z) = 1 - h₃²:\n",
    "\n",
    "δ₃ = (W₄ᵀ·δ₄) ⊙ (1 - h₃²)\n",
    "```\n",
    "\n",
    "Where ⊙ denotes element-wise multiplication.\n",
    "\n",
    "Gradients:\n",
    "```\n",
    "∂L/∂W₃ = δ₃·h₂ᵀ\n",
    "∂L/∂b₃ = δ₃\n",
    "```\n",
    "\n",
    "### 2.6 General Backward Pass Formula\n",
    "\n",
    "For any layer ℓ:\n",
    "\n",
    "```\n",
    "δℓ = (W^T_{ℓ+1}·δ_{ℓ+1}) ⊙ f'(zℓ)\n",
    "\n",
    "∂L/∂Wℓ = δℓ·h^T_{ℓ-1}\n",
    "∂L/∂bℓ = δℓ\n",
    "```\n",
    "\n",
    "This is the **chain rule** in action!\n",
    "\n",
    "### 2.7 Complete Backpropagation Algorithm\n",
    "\n",
    "```\n",
    "1. Forward Pass:\n",
    "   - Compute all z and h values from input to output\n",
    "   - Store them for backward pass\n",
    "\n",
    "2. Compute Output Error:\n",
    "   δ_L = ∂L/∂z_L (depends on loss function)\n",
    "\n",
    "3. Backward Pass (for ℓ = L-1 down to 1):\n",
    "   δℓ = (W^T_{ℓ+1}·δ_{ℓ+1}) ⊙ f'(zℓ)\n",
    "   \n",
    "4. Compute Gradients (for all layers):\n",
    "   ∇Wℓ = δℓ·h^T_{ℓ-1}\n",
    "   ∇bℓ = δℓ\n",
    "\n",
    "5. Update Parameters:\n",
    "   Wℓ ← Wℓ - α·∇Wℓ\n",
    "   bℓ ← bℓ - α·∇bℓ\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Matrix Calculus\n",
    "\n",
    "### 3.1 Derivative of Vector-Vector Product\n",
    "\n",
    "Given:\n",
    "```\n",
    "f(x) = aᵀx\n",
    "```\n",
    "\n",
    "Then:\n",
    "```\n",
    "∂f/∂x = a\n",
    "```\n",
    "\n",
    "### 3.2 Derivative of Quadratic Form\n",
    "\n",
    "Given:\n",
    "```\n",
    "f(x) = xᵀAx\n",
    "```\n",
    "\n",
    "Then:\n",
    "```\n",
    "∂f/∂x = (A + Aᵀ)x\n",
    "```\n",
    "\n",
    "If A is symmetric:\n",
    "```\n",
    "∂f/∂x = 2Ax\n",
    "```\n",
    "\n",
    "### 3.3 Derivative of Matrix Product\n",
    "\n",
    "Given:\n",
    "```\n",
    "f(W) = trace(WᵀAWB)\n",
    "```\n",
    "\n",
    "Then:\n",
    "```\n",
    "∂f/∂W = AWB + AᵀWBᵀ\n",
    "```\n",
    "\n",
    "### 3.4 Practical Example: MSE Loss\n",
    "\n",
    "Given:\n",
    "```\n",
    "L = ||Wx - y||²\n",
    "```\n",
    "\n",
    "Expand:\n",
    "```\n",
    "L = (Wx - y)ᵀ(Wx - y)\n",
    "  = xᵀWᵀWx - 2yᵀWx + yᵀy\n",
    "```\n",
    "\n",
    "Compute gradient:\n",
    "```\n",
    "∂L/∂W = 2Wxxᵀ - 2yxᵀ\n",
    "       = 2(Wx - y)xᵀ\n",
    "       = 2·error·xᵀ\n",
    "```\n",
    "\n",
    "This matches our backpropagation formula!\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Chain Rule for Neural Networks\n",
    "\n",
    "### 4.1 Scalar Chain Rule\n",
    "\n",
    "For y = f(g(x)):\n",
    "```\n",
    "dy/dx = (dy/dg) · (dg/dx)\n",
    "```\n",
    "\n",
    "### 4.2 Multivariable Chain Rule\n",
    "\n",
    "For y = f(u₁, u₂, ..., uₙ) where each uᵢ = gᵢ(x):\n",
    "\n",
    "```\n",
    "∂y/∂x = Σᵢ (∂y/∂uᵢ) · (∂uᵢ/∂x)\n",
    "```\n",
    "\n",
    "### 4.3 Neural Network Example\n",
    "\n",
    "Consider:\n",
    "```\n",
    "h₁ = tanh(W₁x + b₁)\n",
    "h₂ = tanh(W₂h₁ + b₂)\n",
    "L = ||h₂ - y||²\n",
    "```\n",
    "\n",
    "To find ∂L/∂W₁:\n",
    "\n",
    "```\n",
    "∂L/∂W₁ = (∂L/∂h₂) · (∂h₂/∂h₁) · (∂h₁/∂W₁)\n",
    "```\n",
    "\n",
    "Step by step:\n",
    "\n",
    "**Step 1**: ∂L/∂h₂\n",
    "```\n",
    "∂L/∂h₂ = 2(h₂ - y)\n",
    "```\n",
    "\n",
    "**Step 2**: ∂h₂/∂h₁\n",
    "```\n",
    "h₂ = tanh(W₂h₁ + b₂)\n",
    "\n",
    "∂h₂/∂h₁ = tanh'(W₂h₁ + b₂) · W₂\n",
    "        = (1 - h₂²) · W₂\n",
    "```\n",
    "\n",
    "**Step 3**: ∂h₁/∂W₁\n",
    "```\n",
    "h₁ = tanh(W₁x + b₁)\n",
    "\n",
    "∂h₁/∂W₁ = tanh'(W₁x + b₁) ⊗ x\n",
    "        = (1 - h₁²) ⊗ x\n",
    "```\n",
    "\n",
    "**Combine**:\n",
    "```\n",
    "∂L/∂W₁ = [(2(h₂ - y)) · ((1 - h₂²) · W₂)] ⊗ [(1 - h₁²) ⊗ x]\n",
    "```\n",
    "\n",
    "This is exactly what backpropagation computes efficiently!\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Convergence Analysis\n",
    "\n",
    "### 5.1 Convex Optimization\n",
    "\n",
    "For convex functions (bowl-shaped), gradient descent guarantees convergence to global minimum.\n",
    "\n",
    "**Definition**: Function f is convex if:\n",
    "```\n",
    "f(λx + (1-λ)y) ≤ λf(x) + (1-λ)f(y)  for all λ ∈ [0,1]\n",
    "```\n",
    "\n",
    "**Property**: For convex f with L-Lipschitz gradient:\n",
    "```\n",
    "||∇f(x) - ∇f(y)|| ≤ L||x - y||\n",
    "```\n",
    "\n",
    "### 5.2 Convergence Rate\n",
    "\n",
    "With learning rate α = 1/L:\n",
    "\n",
    "```\n",
    "f(x_t) - f(x*) ≤ (2L||x₀ - x*||²) / t\n",
    "```\n",
    "\n",
    "This is **O(1/t)** convergence rate.\n",
    "\n",
    "### 5.3 Non-Convex Case (Neural Networks)\n",
    "\n",
    "Neural networks are **non-convex**:\n",
    "- Multiple local minima\n",
    "- Saddle points\n",
    "- No global convergence guarantee\n",
    "\n",
    "**But in practice**:\n",
    "- SGD with momentum often finds good solutions\n",
    "- Over-parameterization helps (more parameters than data)\n",
    "- Initialization matters (Xavier, He initialization)\n",
    "\n",
    "### 5.4 Learning Rate Selection\n",
    "\n",
    "**Too large**: Divergence\n",
    "```\n",
    "θ_{t+1} = θ_t - α·∇L(θ_t)\n",
    "\n",
    "If α > 2/L, the update overshoots and oscillates\n",
    "```\n",
    "\n",
    "**Too small**: Slow convergence\n",
    "```\n",
    "Takes O(1/α) iterations to reach minimum\n",
    "```\n",
    "\n",
    "**Optimal**: α ∈ [1/L, 2/L]\n",
    "\n",
    "**Adaptive methods** (Adam, RMSprop):\n",
    "- Adjust α per parameter\n",
    "- Use gradient history\n",
    "- More robust to poor initialization\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Advanced Topics\n",
    "\n",
    "### 6.1 Second-Order Methods\n",
    "\n",
    "Newton's method uses curvature information:\n",
    "\n",
    "```\n",
    "θ_{t+1} = θ_t - H⁻¹·∇L(θ_t)\n",
    "```\n",
    "\n",
    "Where H is the Hessian matrix:\n",
    "```\n",
    "H = ∇²L(θ) = [∂²L/∂θᵢ∂θⱼ]\n",
    "```\n",
    "\n",
    "**Pros**: Faster convergence (quadratic)\n",
    "**Cons**: Computing H⁻¹ is O(n³), impractical for large n\n",
    "\n",
    "### 6.2 Quasi-Newton Methods\n",
    "\n",
    "Approximate H⁻¹ without computing it:\n",
    "- BFGS\n",
    "- L-BFGS (limited memory)\n",
    "\n",
    "Used in traditional ML but not deep learning (too many parameters).\n",
    "\n",
    "### 6.3 Momentum\n",
    "\n",
    "SGD with momentum:\n",
    "\n",
    "```\n",
    "v_{t+1} = β·v_t + ∇L(θ_t)\n",
    "θ_{t+1} = θ_t - α·v_{t+1}\n",
    "```\n",
    "\n",
    "**Intuition**: Ball rolling downhill\n",
    "- Accumulates velocity\n",
    "- Reduces oscillation\n",
    "- Faster convergence\n",
    "\n",
    "### 6.4 Adam Optimizer\n",
    "\n",
    "Combines momentum with adaptive learning rates:\n",
    "\n",
    "```\n",
    "m_t = β₁·m_{t-1} + (1-β₁)·g_t          # First moment\n",
    "v_t = β₂·v_{t-1} + (1-β₂)·g_t²         # Second moment\n",
    "\n",
    "m̂_t = m_t / (1 - β₁ᵗ)                   # Bias correction\n",
    "v̂_t = v_t / (1 - β₂ᵗ)                   # Bias correction\n",
    "\n",
    "θ_t = θ_{t-1} - α·m̂_t / (√v̂_t + ε)\n",
    "```\n",
    "\n",
    "**Default hyperparameters**:\n",
    "- α = 0.001\n",
    "- β₁ = 0.9\n",
    "- β₂ = 0.999\n",
    "- ε = 10⁻⁸\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Practical Considerations\n",
    "\n",
    "### 7.1 Gradient Checking\n",
    "\n",
    "Verify backpropagation implementation:\n",
    "\n",
    "```python\n",
    "def gradient_check(f, x, epsilon=1e-5):\n",
    "    \"\"\"\n",
    "    Compare analytical gradient with numerical gradient.\n",
    "    \"\"\"\n",
    "    analytical_grad = compute_gradient(f, x)\n",
    "    \n",
    "    numerical_grad = np.zeros_like(x)\n",
    "    for i in range(len(x)):\n",
    "        x_plus = x.copy()\n",
    "        x_plus[i] += epsilon\n",
    "        x_minus = x.copy()\n",
    "        x_minus[i] -= epsilon\n",
    "        \n",
    "        numerical_grad[i] = (f(x_plus) - f(x_minus)) / (2*epsilon)\n",
    "    \n",
    "    relative_error = np.abs(analytical_grad - numerical_grad) / \\\n",
    "                     (np.abs(analytical_grad) + np.abs(numerical_grad))\n",
    "    \n",
    "    if relative_error < 1e-7:\n",
    "        print(\"✓ Gradient check passed!\")\n",
    "    else:\n",
    "        print(\"✗ Gradient check failed!\")\n",
    "```\n",
    "\n",
    "### 7.2 Numerical Stability\n",
    "\n",
    "**Problem**: Exponentials can overflow/underflow\n",
    "\n",
    "**Solution**: Use numerically stable implementations:\n",
    "\n",
    "```python\n",
    "# Bad: Direct computation\n",
    "def softmax_bad(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "# Good: Subtract max for stability\n",
    "def softmax_good(x):\n",
    "    x_shifted = x - np.max(x)\n",
    "    return np.exp(x_shifted) / np.sum(np.exp(x_shifted))\n",
    "```\n",
    "\n",
    "### 7.3 Vanishing Gradients\n",
    "\n",
    "**Problem**: In deep networks, gradients become exponentially small.\n",
    "\n",
    "**Why**: Repeated multiplication by W and σ'(z):\n",
    "```\n",
    "∂L/∂W₁ ∝ σ'(z₁)·W₂·σ'(z₂)·W₃·...·W_L\n",
    "\n",
    "If σ'(z) < 1 and ||W|| < 1, product vanishes\n",
    "```\n",
    "\n",
    "**Solutions**:\n",
    "1. Use ReLU instead of sigmoid/tanh\n",
    "2. Batch normalization\n",
    "3. Residual connections (skip connections)\n",
    "4. Careful initialization\n",
    "\n",
    "### 7.4 Exploding Gradients\n",
    "\n",
    "**Problem**: Gradients become exponentially large.\n",
    "\n",
    "**Why**: ||W|| > 1 and repeated multiplication\n",
    "\n",
    "**Solutions**:\n",
    "1. Gradient clipping\n",
    "2. Weight regularization\n",
    "3. Proper initialization\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Summary of Key Formulas\n",
    "\n",
    "### Gradient Descent\n",
    "```\n",
    "θ_{t+1} = θ_t - α·∇L(θ_t)\n",
    "```\n",
    "\n",
    "### Backpropagation\n",
    "```\n",
    "δ_L = ∂L/∂z_L\n",
    "δℓ = (W^T_{ℓ+1}·δ_{ℓ+1}) ⊙ σ'(zℓ)\n",
    "∇Wℓ = δℓ·h^T_{ℓ-1}\n",
    "```\n",
    "\n",
    "### Activation Functions\n",
    "```\n",
    "tanh(z) = (e^z - e^(-z)) / (e^z + e^(-z))\n",
    "tanh'(z) = 1 - tanh²(z)\n",
    "\n",
    "ReLU(z) = max(0, z)\n",
    "ReLU'(z) = 1 if z > 0 else 0\n",
    "\n",
    "sigmoid(z) = 1 / (1 + e^(-z))\n",
    "sigmoid'(z) = sigmoid(z)·(1 - sigmoid(z))\n",
    "```\n",
    "\n",
    "### Loss Functions\n",
    "```\n",
    "MSE: L = (1/2n)Σ||ŷᵢ - yᵢ||²\n",
    "∂L/∂ŷ = (1/n)(ŷ - y)\n",
    "\n",
    "Cross-entropy: L = -Σyᵢ log(ŷᵢ)\n",
    "∂L/∂ŷ = -(y/ŷ)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Further Reading\n",
    "\n",
    "### Books\n",
    "1. \"Deep Learning\" - Goodfellow, Bengio, Courville (Chapter 6-8)\n",
    "2. \"Neural Networks for Pattern Recognition\" - Christopher Bishop\n",
    "3. \"Convex Optimization\" - Boyd & Vandenberghe\n",
    "\n",
    "### Papers\n",
    "1. \"Backpropagation Applied to Handwritten Zip Code Recognition\" - LeCun (1989)\n",
    "2. \"Understanding the difficulty of training deep feedforward neural networks\" - Glorot & Bengio (2010)\n",
    "3. \"Adam: A Method for Stochastic Optimization\" - Kingma & Ba (2014)\n",
    "\n",
    "### Online Resources\n",
    "1. Stanford CS231n: http://cs231n.stanford.edu\n",
    "2. Calculus on Computational Graphs: http://colah.github.io\n",
    "3. Neural Networks and Deep Learning: http://neuralnetworksanddeeplearning.com\n",
    "\n",
    "---\n",
    "\n",
    "**Remember**: Understanding these mathematical foundations deeply will set you apart in interviews and enable you to debug and improve ML systems effectively!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d79751",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
